% Chapter 1

\chapter{Introduction} % Main chapter title

\label{Introduction} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content 
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{\bfseries#1}}
\newcommand{\option}[1]{\texttt{\itshape#1}}

%----------------------------------------------------------------------------------------

The automated design of neural network architectures has emerged as a key frontier in modern machine learning, driven by the growing complexity of tasks and the scale of available data. Neural Architecture Search (NAS)~\cite{elsken2019neural} has enabled researchers to systematically explore vast design spaces, moving beyond manual trial-and-error to discover architectures that strike an optimal balance between performance and efficiency. Despite this progress, many neural network architectures, such as Multi-Layer Perceptrons (MLPs)~\cite{rumelhart1986learning}, remain limited by conventional connectivity patterns that restrict information flow to simple, hierarchical pathways.

This thesis aims to challenge and expand this architectural paradigm. It introduces Sarosh’s Perceptron Networks (SPNs), a novel approach that breaks free from the rigid layer-by-layer connectivity of traditional MLPs and allows neurons more freedom in forming cross-layer connection that lead to more complex architectures. By allowing for more flexible and expressive patterns of neuron connectivity, SPNs seek to unlock new levels of model capability and generalization. This work investigates whether such architectural freedom can yield meaningful improvements in performance and efficiency, and examines the implications for the future of neural network architecture design and the field of NAS.

\section{Background and Motivation}
Neural Architecture Search (NAS)~\cite{elsken2019neural} has emerged as a prominent research area in machine learning, aiming to automate the design process of neural network architectures. NAS systematically explores vast spaces of possible network configurations to discover architectures that optimally balance performance, computational cost, and efficiency for specific tasks. This exploration typically involves methods such as reinforcement learning, evolutionary algorithms, and gradient-based optimization, each strategically navigating a complex, multidimensional search space to find optimal architectural solutions.

At the foundational level of neural network architectures lies the perceptron~\cite{rosenblatt1958perceptron}, introduced by Frank Rosenblatt in 1958. As one of the earliest and most influential machine learning models, the perceptron serves as a linear binary classifier, inspired by biological neurons. However, while effective for linearly separable problems, perceptrons inherently struggle to capture complex, non-linear patterns.

To overcome this limitation, Multi-Layer Perceptrons (MLPs)~\cite{rumelhart1986learning} were developed, adding multiple interconnected layers of perceptrons to facilitate learning non-linear relationships. MLPs became capable of effectively training deep architectures through gradient-based optimization. Despite advancements leading to specialized architectures such as Convolutional Neural Networks (CNNs)~\cite{krizhevsky2012imagenet} for image processing and Transformer-based Large Language Models (LLMs)~\cite{vaswani2017attention} for natural language tasks, MLPs continue to serve as a fundamental building block within neural network research.

NAS methodologies have been applied extensively to refine MLP architectures, systematically tuning hyperparameters such as neuron count, layer depth, and activation functions. However, traditional MLP architectures have typically constrained their search spaces to hierarchical layer-to-layer connectivity, inherently neglecting more flexible connectivity patterns such as non-sequential cross layer connectivity and internal connections between neurons in the same layer. While architectures like Residual Networks (ResNet)~\cite{he2016deep} have introduced skip connections to partially mitigate this limitation, the potential of unrestricted neuron connectivity within MLP architectures remains largely unexplored.

\section{Problem Statement}

The SPNs introduced in this thesis provide a novel approach designed to unlock the unexplored connectivity potential within traditional MLP search spaces. SPNs remove the conventional constraints of strict hierarchical connections, enabling more flexible interactions between neurons across the entire network. By facilitating direct neuron-to-neuron connections irrespective of layer boundaries, SPNs aim to enhance neural network expressiveness and improve model performance significantly.

Critically, SPNs seek to expand architectural expressivity without incurring substantial computational overhead or sacrificing training efficiency. This research systematically investigates whether leveraging the expanded search space provided by SPNs can achieve meaningful performance improvements compared to traditional MLPs. By thoroughly exploring the advantages offered by this approach, this thesis aims to provide new insights and methodologies that could significantly impact the design and optimization of neural network architectures.

%----------------------------------------------------------------------------------------

\section{Research Gap and Contributions}

There is a significant gap in the existing literature regarding the optimization of neural network architectures, particularly concerning neuron connectivity. Traditional MLPs rely on rigid, fixed structures that inherently limit the interactions among neurons across different layers. Although ResNet~\cite{he2016deep} introduced skip connections to address some of these limitations, it still imposes restrictive conditions, such as requiring consistent dimensionality across residually connected layers or limiting connections primarily to adjacent layers.

The main contributions of this thesis are:
\begin{enumerate}
\item \textbf{Introduction of Sarosh's Perceptron Networks (SPNs):} A novel neural network architecture framework that removes the connectivity constraints inherent to traditional MLP designs.
\item \textbf{Empirical Evaluation:} A comprehensive comparison of SPNs with conventional MLPs to investigate whether enhanced neuron connectivity translates into improved model accuracy, training speed, and efficiency.
\item \textbf{Enhanced Training Efficiency:} An investigation into the potential of SPNs to achieve superior performance while mitigating the computational and memory costs typically associated with increasing complexity and parameters in neural network architectures.
\end{enumerate}

%----------------------------------------------------------------------------------------

\section{Research Questions}

\begin{enumerate}[label={RQ\arabic*.}, leftmargin=*]
    \item Does increasing connection density in Sarosh’s Perceptron Networks (SPNs), while keeping the layer count and design identical to MLPs, lead to better model performance? \label{RQ1}
    \item Does removing connectivity restrictions in MLP architectures (allowing arbitrary neuron-to-neuron connections) enhance the network’s learning capability or representational power? \label{RQ2}
    \item Can SPNs with fewer layers but higher connection density match or exceed the performance of deeper MLPs, thereby improving throughput efficiency and reducing training times? \label{RQ3}
    \item How do SPNs compare to MLPs in terms of training time, computational efficiency, memory usage, and predictive accuracy across a range of datasets and tasks? \label{RQ4}
    \item Does the increased architectural flexibility of SPNs improve their ability to generalize across diverse datasets and tasks compared to standard MLPs? \label{RQ5}
    \item After maximizing connections in SPNs, can pruning strategies maintain high predictive performance while significantly improving computational efficiency and training time? \label{RQ6}
\end{enumerate}

%----------------------------------------------------------------------------------------

\section{Scope and Limitations}

This research focuses on SPN's potential to enhance neural network performance through increased neuron connectivity. The primary scope of this thesis is the empirical evaluation of SPNs in comparison to traditional MLPs, with all experiments limited to supervised classification tasks.

Key limitations include:

\begin{enumerate}
\item \textbf{Computational Resources:}By design, SPNs introduce a significantly larger number of connections than standard MLPs, resulting in increased computational demands for both training and inference. This research primarily evaluates whether the potential improvements in model performance and expressiveness are sufficient to justify the additional computational cost. However, due to hardware constraints, some larger-scale configurations or extended hyperparameter sweeps may be beyond the feasible scope of experimentation.
\item \textbf{Task and Domain Specificity:} The experiments and comparisons in this study are restricted to a selection of well-known benchmark datasets for classification. As such, the observed results may be influenced by the characteristics of these datasets, and the findings may not fully generalize to other domains (such as regression, reinforcement learning, or unsupervised tasks) or to highly complex, real-world data distributions.
\item \textbf{Scope of Architectural Exploration:} The present study is limited to comparisons between SPNs and traditional MLPs. It does not extend to specialized neural network architectures such as CNNs, ResNets, Recurrent Neural Networks (RNNs)~\cite{lipton2015critical}, Vision Transformers (ViTs)~\cite{dosovitskiy2020image}, or LLMs. As a result, the boundaries of SPN expressiveness and their potential for integration with these advanced architectures remain unexplored. Further research will be required to evaluate the applicability and benefits of SPNs in the context of these specialized models.
\end{enumerate}

%----------------------------------------------------------------------------------------


\section{Thesis Structure and Overview}

The remainder of this thesis is organized as follows:
\begin{enumerate}
    \item Chapter \ref{LiteratureReview} presents a comprehensive literature review, covering traditional Multi-Layer Perceptrons (MLPs), neural architecture search (NAS), ResNets and pruning methods and the Lottery Ticket Hypothesis.
    \item Chapter \ref{Methodology} details the methodology used in this research, including the design and implementation of SPNs.
    \item Chapter \ref{Experiments} describes the the experimental setup, dataset selection, model selection, training procedures, and evaluation metrics.
    \item Chapter \ref{Results} presents the results and analysis of empirical comparisons between SPNs and traditional MLPs across multiple benchmark classification datasets.
    \item Chapter \ref{Conclusion} concludes the thesis by summarizing the key findings, and proposing directions for future research in neural architecture optimization and SPN applications.
\end{enumerate}