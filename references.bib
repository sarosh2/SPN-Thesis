@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{rae2021scaling,
  title={Scaling language models: Methods, analysis \& insights from training gopher},
  author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
  journal={arXiv preprint arXiv:2112.11446},
  year={2021}
}

@article{wang2023knowledge,
  title={Knowledge editing for large language models: A survey},
  author={Wang, Song and Zhu, Yaochen and Liu, Haochen and Zheng, Zaiyi and Chen, Chen and others},
  journal={arXiv preprint arXiv:2310.16218},
  year={2023}
}

@article{wu2024continual,
  title={Continual learning for large language models: A survey},
  author={Wu, Tongtong and Luo, Linhao and Li, Yuan-Fang and Pan, Shirui and Vu, Thuy-Trang and Haffari, Gholamreza},
  journal={arXiv preprint arXiv:2402.01364},
  year={2024}
}

@article{van2022three,
  title={Three types of incremental learning},
  author={Van de Ven, Gido M and Tuytelaars, Tinne and Tolias, Andreas S},
  journal={Nature Machine Intelligence},
  volume={4},
  number={12},
  pages={1185--1197},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@article{luo2023empirical,
  title={An empirical study of catastrophic forgetting in large language models during continual fine-tuning},
  author={Luo, Yun and Yang, Zhen and Meng, Fandong and Li, Yafu and Zhou, Jie and Zhang, Yue},
  journal={arXiv preprint arXiv:2308.08747},
  year={2023}
}

@article{de2021continual,
  title={A continual learning survey: Defying forgetting in classification tasks},
  author={De Lange, Matthias and Aljundi, Rahaf and Masana, Marc and Parisot, Sarah and Jia, Xu and Leonardis, Ale{\v{s}} and Slabaugh, Gregory and Tuytelaars, Tinne},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={44},
  number={7},
  pages={3366--3385},
  year={2021},
  publisher={IEEE}
}

@article{zhang2024core,
  title={CORE: Mitigating Catastrophic Forgetting in Continual Learning through Cognitive Replay},
  author={Zhang, Jianshu and Fu, Yankai and Peng, Ziheng and Yao, Dongyu and He, Kun},
  journal={arXiv preprint arXiv:2402.01348},
  year={2024}
}

@article{sheng2024slora,
  title={SLoRA: Scalable Serving of Thousands of LoRA Adapters},
  author={Sheng, Ying and Cao, Shiyi and Li, Dacheng and Hooper, Coleman and Lee, Nicholas and Yang, Shuo and Chou, Christopher and Zhu, Banghua and Zheng, Lianmin and Keutzer, Kurt and others},
  journal={Proceedings of Machine Learning and Systems},
  volume={6},
  pages={296--311},
  year={2024}
}

@article{dou2023loramoe,
  title={Loramoe: Revolutionizing mixture of experts for maintaining world knowledge in language model alignment},
  author={Dou, Shihan and Zhou, Enyu and Liu, Yan and Gao, Songyang and Zhao, Jun and Shen, Wei and Zhou, Yuhao and Xi, Zhiheng and Wang, Xiao and Fan, Xiaoran and others},
  journal={arXiv preprint arXiv:2312.09979},
  year={2023}
}

@article{wang2024comprehensive,
  title={A comprehensive survey of continual learning: Theory, method and application},
  author={Wang, Liyuan and Zhang, Xingxing and Su, Hang and Zhu, Jun},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2024},
  publisher={IEEE}
}

@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{shen2023large,
  title={Large language model alignment: A survey},
  author={Shen, Tianhao and Jin, Renren and Huang, Yufei and Liu, Chuang and Dong, Weilong and Guo, Zishan and Wu, Xinwei and Liu, Yan and Xiong, Deyi},
  journal={arXiv preprint arXiv:2309.15025},
  year={2023}
}

@article{sun2024principle,
  title={Principle-driven self-alignment of language models from scratch with minimal human supervision},
  author={Sun, Zhiqing and Shen, Yikang and Zhou, Qinhong and Zhang, Hongxin and Chen, Zhenfang and Cox, David and Yang, Yiming and Gan, Chuang},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{hu2023meetingbank,
  title={MeetingBank: A benchmark dataset for meeting summarization},
  author={Hu, Yebowen and Ganter, Tim and Deilamsalehy, Hanieh and Dernoncourt, Franck and Foroosh, Hassan and Liu, Fei},
  journal={arXiv preprint arXiv:2305.17529},
  year={2023}
}

@article{lu2022learn,
  title={Learn to explain: Multimodal reasoning via thought chains for science question answering},
  author={Lu, Pan and Mishra, Swaroop and Xia, Tanglin and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Kalyan, Ashwin},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={2507--2521},
  year={2022}
}

@article{mishra2022numglue,
  title={NumGLUE: A suite of fundamental yet challenging mathematical reasoning tasks},
  author={Mishra, Swaroop and Mitra, Arindam and Varshney, Neeraj and Sachdeva, Bhavdeep and Clark, Peter and Baral, Chitta and Kalyan, Ashwin},
  journal={arXiv preprint arXiv:2204.05660},
  year={2022}
}

@article{wang2023trace,
  title={TRACE: A Comprehensive Benchmark for Continual Learning in Large Language Models},
  author={Wang, Xiao and Zhang, Yuansen and Chen, Tianze and Gao, Songyang and Jin, Senjie and Yang, Xianjun and Xi, Zhiheng and Zheng, Rui and Zou, Yicheng and Gui, Tao and others},
  journal={arXiv preprint arXiv:2310.06762},
  year={2023}
}

@article{chung2024scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={Journal of Machine Learning Research},
  volume={25},
  number={70},
  pages={1--53},
  year={2024}
}

@article{guo2024deepseek,
  title={DeepSeek-Coder: When the Large Language Model Meets Programming--The Rise of Code Intelligence},
  author={Guo, Daya and Zhu, Qihao and Yang, Dejian and Xie, Zhenda and Dong, Kai and Zhang, Wentao and Chen, Guanting and Bi, Xiao and Wu, Yu and Li, YK and others},
  journal={arXiv preprint arXiv:2401.14196},
  year={2024}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde De Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

@article{ren2020codebleu,
  title={Codebleu: a method for automatic evaluation of code synthesis},
  author={Ren, Shuo and Guo, Daya and Lu, Shuai and Zhou, Long and Liu, Shujie and Tang, Duyu and Sundaresan, Neel and Zhou, Ming and Blanco, Ambrosio and Ma, Shuai},
  journal={arXiv preprint arXiv:2009.10297},
  year={2020}
}

@article{chang2024survey,
  title={A survey on evaluation of large language models},
  author={Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and others},
  journal={ACM Transactions on Intelligent Systems and Technology},
  volume={15},
  number={3},
  pages={1--45},
  year={2024},
  publisher={ACM New York, NY}
}
@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}

@article{wu2023online,
  title={Online Continual Knowledge Learning for Language Models},
  author={Wu, Yuhao and Shi, Tongjun and Sharma, Karthick and Seah, Chun Wei and Zhang, Shuhao},
  journal={arXiv preprint arXiv:2311.09632},
  year={2023}
}

@article{clark2019boolq,
  title={BoolQ: Exploring the surprising difficulty of natural yes/no questions},
  author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1905.10044},
  year={2019}
}

@article{wang2018glue,
  title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}

@article{zellers2019hellaswag,
  title={Hellaswag: Can a machine really finish your sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  journal={arXiv preprint arXiv:1905.07830},
  year={2019}
}

@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@article{zhang2024careful,
  title={A careful examination of large language model performance on grade school arithmetic},
  author={Zhang, Hugh and Da, Jeff and Lee, Dean and Robinson, Vaughn and Wu, Catherine and Song, Will and Zhao, Tiffany and Raja, Pranav and Slack, Dylan and Lyu, Qin and others},
  journal={arXiv preprint arXiv:2405.00332},
  year={2024}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}

@article{xu2023parameter,
  title={Parameter-efficient fine-tuning methods for pretrained language models: A critical review and assessment},
  author={Xu, Lingling and Xie, Haoran and Qin, Si-Zhao Joe and Tao, Xiaohui and Wang, Fu Lee},
  journal={arXiv preprint arXiv:2312.12148},
  year={2023}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}

@article{minaee2024large,
  title={Large language models: A survey},
  author={Minaee, Shervin and Mikolov, Tomas and Nikzad, Narjes and Chenaghlu, Meysam and Socher, Richard and Amatriain, Xavier and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2402.06196},
  year={2024}
}

@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{naveed2023comprehensive,
  title={A comprehensive overview of large language models},
  author={Naveed, Humza and Khan, Asad Ullah and Qiu, Shi and Saqib, Muhammad and Anwar, Saeed and Usman, Muhammad and Barnes, Nick and Mian, Ajmal},
  journal={arXiv preprint arXiv:2307.06435},
  year={2023}
}

@article{shi2024continual,
  title={Continual learning of large language models: A comprehensive survey},
  author={Shi, Haizhou and Xu, Zihao and Wang, Hengyi and Qin, Weiyi and Wang, Wenyuan and Wang, Yibin and Wang, Hao},
  journal={arXiv preprint arXiv:2404.16789},
  year={2024}
}

@article{dong2019unified,
  title={Unified language model pre-training for natural language understanding and generation},
  author={Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{parisi2019continual,
  title={Continual lifelong learning with neural networks: A review},
  author={Parisi, German I and Kemker, Ronald and Part, Jose L and Kanan, Christopher and Wermter, Stefan},
  journal={Neural networks},
  volume={113},
  pages={54--71},
  year={2019},
  publisher={Elsevier}
}

@article{gogoulou2023study,
  title={A study of continual learning under language shift},
  author={Gogoulou, Evangelia and Lesort, Timoth{\'e}e and Boman, Magnus and Nivre, Joakim},
  journal={arXiv preprint arXiv:2311.01200},
  year={2023}
}

@article{kotha2023understanding,
  title={Understanding catastrophic forgetting in language models via implicit inference},
  author={Kotha, Suhas and Springer, Jacob Mitchell and Raghunathan, Aditi},
  journal={arXiv preprint arXiv:2309.10105},
  year={2023}
}

@article{von2019continual,
  title={Continual learning with hypernetworks},
  author={Von Oswald, Johannes and Henning, Christian and Grewe, Benjamin F and Sacramento, Jo{\~a}o},
  journal={arXiv preprint arXiv:1906.00695},
  year={2019}
}


@inproceedings{rebuffi2017icarl,
  title={icarl: Incremental classifier and representation learning},
  author={Rebuffi, Sylvestre-Alvise and Kolesnikov, Alexander and Sperl, Georg and Lampert, Christoph H},
  booktitle={Proceedings of the IEEE conference on Computer Vision and Pattern Recognition},
  pages={2001--2010},
  year={2017}
}

@article{wang2023comprehensive,
  title={A comprehensive survey of forgetting in deep learning beyond continual learning},
  author={Wang, Zhenyi and Yang, Enneng and Shen, Li and Huang, Heng},
  journal={arXiv preprint arXiv:2307.09218},
  year={2023}
}

@incollection{mccloskey1989catastrophic,
  title={Catastrophic interference in connectionist networks: The sequential learning problem},
  author={McCloskey, Michael and Cohen, Neal J},
  booktitle={Psychology of learning and motivation},
  volume={24},
  pages={109--165},
  year={1989},
  publisher={Elsevier}
}

@article{chaudhry2019tiny,
  title={On tiny episodic memories in continual learning},
  author={Chaudhry, Arslan and Rohrbach, Marcus and Elhoseiny, Mohamed and Ajanthan, Thalaiyasingam and Dokania, Puneet K and Torr, Philip HS and Ranzato, Marc'Aurelio},
  journal={arXiv preprint arXiv:1902.10486},
  year={2019}
}

@article{shin2017continual,
  title={Continual learning with deep generative replay},
  author={Shin, Hanul and Lee, Jung Kwon and Kim, Jaehong and Kim, Jiwon},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{buzzega2020dark,
  title={Dark experience for general continual learning: a strong, simple baseline},
  author={Buzzega, Pietro and Boschini, Matteo and Porrello, Angelo and Abati, Davide and Calderara, Simone},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={15920--15930},
  year={2020}
}

@article{scialom2022fine,
  title={Fine-tuned language models are continual learners},
  author={Scialom, Thomas and Chakrabarty, Tuhin and Muresan, Smaranda},
  journal={arXiv preprint arXiv:2205.12393},
  year={2022}
}


@article{song2023conpet,
  title={Conpet: Continual parameter-efficient tuning for large language models},
  author={Song, Chenyang and Han, Xu and Zeng, Zheni and Li, Kuai and Chen, Chen and Liu, Zhiyuan and Sun, Maosong and Yang, Tao},
  journal={arXiv preprint arXiv:2309.14763},
  year={2023}
}

@article{dong2023abilities,
  title={How abilities in large language models are affected by supervised fine-tuning data composition},
  author={Dong, Guanting and Yuan, Hongyi and Lu, Keming and Li, Chengpeng and Xue, Mingfeng and Liu, Dayiheng and Wang, Wei and Yuan, Zheng and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2310.05492},
  year={2023}
}

@article{lopez2017gradient,
  title={Gradient episodic memory for continual learning},
  author={Lopez-Paz, David and Ranzato, Marc'Aurelio},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{chaudhry2018efficient,
  title={Efficient lifelong learning with a-gem},
  author={Chaudhry, Arslan and Ranzato, Marc'Aurelio and Rohrbach, Marcus and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:1812.00420},
  year={2018}
}

@inproceedings{farajtabar2020orthogonal,
  title={Orthogonal gradient descent for continual learning},
  author={Farajtabar, Mehrdad and Azizan, Navid and Mott, Alex and Li, Ang},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3762--3773},
  year={2020},
  organization={PMLR}
}

@article{zeng2019continual,
  title={Continual learning of context-dependent processing in neural networks},
  author={Zeng, Guanxiong and Chen, Yang and Cui, Bo and Yu, Shan},
  journal={Nature Machine Intelligence},
  volume={1},
  number={8},
  pages={364--372},
  year={2019},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{guo2022adaptive,
  title={Adaptive orthogonal projection for batch and online continual learning},
  author={Guo, Yiduo and Hu, Wenpeng and Zhao, Dongyan and Liu, Bing},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={6},
  pages={6783--6791},
  year={2022}
}

@article{li2017learning,
  title={Learning without forgetting},
  author={Li, Zhizhong and Hoiem, Derek},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={40},
  number={12},
  pages={2935--2947},
  year={2017},
  publisher={IEEE}
}

@article{kirkpatrick2017overcoming,
  title={Overcoming catastrophic forgetting in neural networks},
  author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},
  journal={Proceedings of the national academy of sciences},
  volume={114},
  number={13},
  pages={3521--3526},
  year={2017},
  publisher={National Acad Sciences}
}

@inproceedings{schwarz2018progress,
  title={Progress \& compress: A scalable framework for continual learning},
  author={Schwarz, Jonathan and Czarnecki, Wojciech and Luketina, Jelena and Grabska-Barwinska, Agnieszka and Teh, Yee Whye and Pascanu, Razvan and Hadsell, Raia},
  booktitle={International conference on machine learning},
  pages={4528--4537},
  year={2018},
  organization={PMLR}
}

@article{nguyen2017variational,
  title={Variational continual learning},
  author={Nguyen, Cuong V and Li, Yingzhen and Bui, Thang D and Turner, Richard E},
  journal={arXiv preprint arXiv:1710.10628},
  year={2017}
}


@article{rusu2016progressive,
  title={Progressive neural networks},
  author={Rusu, Andrei A and Rabinowitz, Neil C and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick, James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia},
  journal={arXiv preprint arXiv:1606.04671},
  year={2016}
}

@article{razdaibiedina2023progressive,
  title={Progressive prompts: Continual learning for language models},
  author={Razdaibiedina, Anastasia and Mao, Yuning and Hou, Rui and Khabsa, Madian and Lewis, Mike and Almahairi, Amjad},
  journal={arXiv preprint arXiv:2301.12314},
  year={2023}
}

@article{wistuba2023continual,
  title={Continual learning with low rank adaptation},
  author={Wistuba, Martin and Sivaprasad, Prabhu Teja and Balles, Lukas and Zappella, Giovanni},
  journal={arXiv preprint arXiv:2311.17601},
  year={2023}
}

@article{sheng2023s,
  title={S-lora: Serving thousands of concurrent lora adapters},
  author={Sheng, Ying and Cao, Shiyi and Li, Dacheng and Hooper, Coleman and Lee, Nicholas and Yang, Shuo and Chou, Christopher and Zhu, Banghua and Zheng, Lianmin and Keutzer, Kurt and others},
  journal={arXiv preprint arXiv:2311.03285},
  year={2023}
}

@article{qin2021lfpt5,
  title={Lfpt5: A unified framework for lifelong few-shot language learning based on prompt tuning of t5},
  author={Qin, Chengwei and Joty, Shafiq},
  journal={arXiv preprint arXiv:2110.07298},
  year={2021}
}

@article{zhao2024dapt,
  title={DAPT: A Dual Attention Framework for Parameter-Efficient Continual Learning of Large Language Models},
  author={Zhao, Weixiang and Wang, Shilong and Hu, Yulin and Zhao, Yanyan and Qin, Bing and Zhang, Xuanyu and Yang, Qing and Xu, Dongliang and Che, Wanxiang},
  journal={arXiv preprint arXiv:2401.08295},
  year={2024}
}

@inproceedings{mok2023large,
  title={Large-scale lifelong learning of in-context instructions and how to tackle it},
  author={Mok, Jisoo and Do, Jaeyoung and Lee, Sungjin and Taghavi, Tara and Yu, Seunghak and Yoon, Sungroh},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={12573--12589},
  year={2023}
}

@article{zhong2024panda,
  title={Panda: Prompt transfer meets knowledge distillation for efficient model adaptation},
  author={Zhong, Qihuang and Ding, Liang and Liu, Juhua and Du, Bo and Tao, Dacheng},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2024},
  publisher={IEEE}
}

@article{wang2023orthogonal,
  title={Orthogonal subspace learning for language model continual learning},
  author={Wang, Xiao and Chen, Tianze and Ge, Qiming and Xia, Han and Bao, Rong and Zheng, Rui and Zhang, Qi and Gui, Tao and Huang, Xuanjing},
  journal={arXiv preprint arXiv:2310.14152},
  year={2023}
}

@inproceedings{gao2023unified,
  title={A unified continual learning framework with general parameter-efficient tuning},
  author={Gao, Qiankun and Zhao, Chen and Sun, Yifan and Xi, Teng and Zhang, Gang and Ghanem, Bernard and Zhang, Jian},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={11483--11493},
  year={2023}
}

@article{zhang2019root,
  title={Root mean square layer normalization},
  author={Zhang, Biao and Sennrich, Rico},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}

@article{guo2024efficient,
  title={Efficient Continual Pre-training by Mitigating the Stability Gap},
  author={Guo, Yiduo and Fu, Jie and Zhang, Huishuai and Zhao, Dongyan and Shen, Yikang},
  journal={arXiv preprint arXiv:2406.14833},
  year={2024}
}